{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3ea6d3",
   "metadata": {},
   "source": [
    "In this section, we first introduce the general paradigm of horizontal FL and then discuss the corresponding implementation in FLGo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704f558",
   "metadata": {},
   "source": [
    "# 2.1.1 Classical Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d372c65",
   "metadata": {},
   "source": [
    "![Figure_fedprocess](https://raw.githubusercontent.com/WwZzz/myfigs/master/figure_federated_process_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e83e0a",
   "metadata": {},
   "source": [
    "In a classical horizontal FL scene, there is a center server that coordinates clients to collaboratively train a global model iteratively. In each iteration, the server first samples a subset from all the clients. Then, the server broadcasts the global model the selected clients. After receiving the global model, the clients locally train it with local data. Finally, the clients send back the updated models to the server and the server aggregates the models into the new global model. The whole process is as shown in the figure above. Existing methods usually improve one or more of the five steps to realize various purposes like fairness and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d14d5e",
   "metadata": {},
   "source": [
    "![Figure1](https://raw.githubusercontent.com/WwZzz/myfigs/master/overview_flgo_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c7d65",
   "metadata": {},
   "source": [
    "The cooresponding implementation of the FL process is shown in Figure 2. We use `iterate` function to model the behaviors of the server and `reply` function to model the behaviors of clients when being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ee24f",
   "metadata": {},
   "source": [
    "# 2.1.2 Details of Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa83f88",
   "metadata": {},
   "source": [
    "## Server's Behavior: Server.iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735146fa",
   "metadata": {},
   "source": [
    "The training process starts with `run` method of the server, which starts iterations (i.e. communication rounds) by using a loop. In each iteration of the loop, the server will call `iterate` to carry out each step. A standard implementation of `iterate` (i.e. `flgo.algorithm.fedbase.iterate`) is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645f55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def iterate(self):\n",
    "        \"\"\"\n",
    "        The standard iteration of each federated communication round that contains three\n",
    "        necessary procedure in FL: client selection, communication and model aggregation.\n",
    "\n",
    "        Returns:\n",
    "            False if the global model is not updated in this iteration\n",
    "        \"\"\"\n",
    "        # sample clients: Uniform sampling as default\n",
    "        self.selected_clients = self.sample()\n",
    "        # training\n",
    "        models = self.communicate(self.selected_clients)['model']\n",
    "        # aggregate: pk = ni/sum(ni) as default\n",
    "        self.model = self.aggregate(models)\n",
    "        return len(models) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c1f76",
   "metadata": {},
   "source": [
    "## ① Server.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3786a",
   "metadata": {},
   "source": [
    "During each iteration, the server first sample clients by calling `self.sample()`, which returns a list of clients' IDs. We implement three  sampling strategies in our preset sampling method as below. `full` sampling means selecting all the clients. `uniform` sampling means selecting clients uniformly without replacement. `md` sampling means selecting clients with replacement by probabilities w.r.t. the ratio of data sizes. Improvement on sampling strategies can be adapted here by overwriting `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6777da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sample(self):\n",
    "        r\"\"\"\n",
    "        Sample the clients. There are three types of sampling manners:\n",
    "        full sample, uniform sample without replacement, and MDSample\n",
    "        with replacement. Particularly, if 'available' is in self.sample_option,\n",
    "        the server will only sample from currently available clients.\n",
    "\n",
    "        Returns:\n",
    "            a list of the ids of the selected clients\n",
    "\n",
    "        Example:\n",
    "        ```python\n",
    "            >>> selected_clients=self.sample()\n",
    "            >>> selected_clients\n",
    "            >>> # The selected_clients is a list of clients' ids\n",
    "        ```\n",
    "        \"\"\"\n",
    "        all_clients = self.available_clients if 'available' in self.sample_option else [cid for cid in\n",
    "                                                                                        range(self.num_clients)]\n",
    "        # full sampling with unlimited communication resources of the server\n",
    "        if 'full' in self.sample_option:\n",
    "            return all_clients\n",
    "        # sample clients\n",
    "        elif 'uniform' in self.sample_option:\n",
    "            # original sample proposed by fedavg\n",
    "            selected_clients = list(\n",
    "                np.random.choice(all_clients, min(self.clients_per_round, len(all_clients)), replace=False)) if len(\n",
    "                all_clients) > 0 else []\n",
    "        elif 'md' in self.sample_option:\n",
    "            # the default setting that is introduced by FedProx, where the clients are sampled with the probability in proportion to their local_movielens_recommendation data sizes\n",
    "            local_data_vols = [self.clients[cid].datavol for cid in all_clients]\n",
    "            total_data_vol = sum(local_data_vols)\n",
    "            p = np.array(local_data_vols) / total_data_vol\n",
    "            selected_clients = list(np.random.choice(all_clients, self.clients_per_round, replace=True, p=p)) if len(\n",
    "                all_clients) > 0 else []\n",
    "        return selected_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7c8e5",
   "metadata": {},
   "source": [
    "## ② Communication- Broadcast: Server.pack & Client.unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda1471",
   "metadata": {},
   "source": [
    "The communication process is realized by the method `communicate(client_ids: list[int], mtype: str, asynchronous: bool)`, which contains a full ask&reply process between the server and the clients. The second step only refers to the broadcast-communication, which only describes what the server transmitting to the clients. Therefore, we use two method, `Server.pack(client_id)` and `Client.unpack()` to model the broadcast-communication process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416866f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def pack(self, client_id, mtype=0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Pack the necessary information for the client's local training.\n",
    "        Any operations of compression or encryption should be done here.\n",
    "        :param\n",
    "            client_id: the id of the client to communicate with\n",
    "        :return\n",
    "            a dict that only contains the global model as default.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model\" : copy.deepcopy(self.model),\n",
    "        }\n",
    "    \n",
    "class Client:\n",
    "        def unpack(self, received_pkg):\n",
    "        \"\"\"\n",
    "        Unpack the package received from the server\n",
    "        :param\n",
    "            received_pkg: a dict contains the global model as default\n",
    "        :return:\n",
    "            the unpacked information that can be rewritten\n",
    "        \"\"\"\n",
    "        # unpack the received package\n",
    "        return received_pkg['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b652e14",
   "metadata": {},
   "source": [
    "The transmitted package should be a `dict` in python. The server will send a copy of the global model, and the client will unpack the package to obtain the global model as default. Any changes on the content of the down-streaming packages should be implemented here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f4918",
   "metadata": {},
   "source": [
    "## Clients' Behavior: Client.reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c91f1",
   "metadata": {},
   "source": [
    "After clients receiving the global models, the method `Client.reply` will automatically be triggered to model the clients' behaviors. The implementation of `reply` is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18db3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reply(self, svr_pkg):\n",
    "        r\"\"\"\n",
    "        Reply a package to the server. The whole local_movielens_recommendation procedure should be defined here.\n",
    "        The standard form consists of three procedure: unpacking the\n",
    "        server_package to obtain the global model, training the global model,\n",
    "        and finally packing the updated model into client_package.\n",
    "\n",
    "        Args:\n",
    "            svr_pkg (dict): the package received from the server\n",
    "\n",
    "        Returns:\n",
    "            client_pkg (dict): the package to be send to the server\n",
    "        \"\"\"\n",
    "        model = self.unpack(svr_pkg)\n",
    "        self.train(model)\n",
    "        cpkg = self.pack(model)\n",
    "        return cpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4f083",
   "metadata": {},
   "source": [
    "## ③ Local Training: Client.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6e6fc",
   "metadata": {},
   "source": [
    "The local training is made by the method `Client.train`, which receives a global model as the input and trains it with local data. Any modification on local training procedures should be implemented here. The default implementation is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78de8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, model):\n",
    "        r\"\"\"\n",
    "        Standard local_movielens_recommendation training procedure. Train the transmitted model with\n",
    "        local_movielens_recommendation training dataset.\n",
    "\n",
    "        Args:\n",
    "            model (FModule): the global model\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        optimizer = self.calculator.get_optimizer(model, lr=self.learning_rate, weight_decay=self.weight_decay,\n",
    "                                                  momentum=self.momentum)\n",
    "        for iter in range(self.num_steps):\n",
    "            # get a batch of data\n",
    "            batch_data = self.get_batch_data()\n",
    "            model.zero_grad()\n",
    "            # calculate the loss of the model on batched dataset through task-specified calculator\n",
    "            loss = self.calculator.compute_loss(model, batch_data)['loss']\n",
    "            loss.backward()\n",
    "            if self.clip_grad>0:torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=self.clip_grad)\n",
    "            optimizer.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aefd6a",
   "metadata": {},
   "source": [
    "Particularly, we let the task-spefific calculation be transparent to the optimization algorithms. Therefore, one algorithm (e.g. FedAvg) can be adapted to different types of tasks without any changes. `calculator` is responsible for all the task-specific calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a7eb6",
   "metadata": {},
   "source": [
    "## ④ Communication - Upload: Client.pack & Server.unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c33cc",
   "metadata": {},
   "source": [
    "The communication of uploading models from clients is modeled by `Client.pack(*args, **kwargs)` and `Server.unpack(packages_list)`, which is similar to the step ②. Different from ②, the server as the receiver needs to simultaneously handle a list of packages from different clients. We let `Server.unpack` return the values in the uploaded packages as a dict that shares the same keys with each client's pakcage. Modification on the content of upload-communication should be implemented in `Client.pack` that returns a dict as a package each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc6eccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def unpack(self, packages_received_from_clients):\n",
    "        \"\"\"\n",
    "        Unpack the information from the received packages. Return models and losses as default.\n",
    "        :param\n",
    "            packages_received_from_clients:\n",
    "        :return:\n",
    "            res: collections.defaultdict that contains several lists of the clients' reply\n",
    "        \"\"\"\n",
    "        if len(packages_received_from_clients)==0: return collections.defaultdict(list)\n",
    "        res = {pname:[] for pname in packages_received_from_clients[0]}\n",
    "        for cpkg in packages_received_from_clients:\n",
    "            for pname, pval in cpkg.items():\n",
    "                res[pname].append(pval)\n",
    "        return res\n",
    "        \n",
    "class Client:\n",
    "    def pack(self, model, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Packing the package to be send to the server. The operations of compression\n",
    "        of encryption of the package should be done here.\n",
    "        :param\n",
    "            model: the locally trained model\n",
    "        :return\n",
    "            package: a dict that contains the necessary information for the server\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model\" : model,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2e3cf",
   "metadata": {},
   "source": [
    "## ⑤ Model Aggregation: Server.aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92301c99",
   "metadata": {},
   "source": [
    "The server finally aggregates the received models into a new global model by the method `Server.aggregate(models: list)`. There are four preset aggregation modes in our implementation. And using the normalized ratios of local data sizes (i.e. FedAvg) is set the default aggregatino option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(self, models: list, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Aggregate the locally improved models.\n",
    "        :param\n",
    "            models: a list of local models\n",
    "        :return\n",
    "            the averaged result\n",
    "        pk = nk/n where n=self.data_vol\n",
    "        K = |S_t|\n",
    "        N = |S|\n",
    "        -------------------------------------------------------------------------------------------------------------------------\n",
    "         weighted_scale                 |uniform (default)          |weighted_com (original fedavg)   |other\n",
    "        ==========================================================================================================================\n",
    "        N/K * Σpk * model_k             |1/K * Σmodel_k             |(1-Σpk) * w_old + Σpk * model_k  |Σ(pk/Σpk) * model_k\n",
    "        \"\"\"\n",
    "        if len(models) == 0: return self.model\n",
    "        local_data_vols = [c.datavol for c in self.clients]\n",
    "        total_data_vol = sum(local_data_vols)\n",
    "        if self.aggregation_option == 'weighted_scale':\n",
    "            p = [1.0 * local_data_vols[cid] /total_data_vol for cid in self.received_clients]\n",
    "            K = len(models)\n",
    "            N = self.num_clients\n",
    "            return fmodule._model_sum([model_k * pk for model_k, pk in zip(models, p)]) * N / K\n",
    "        elif self.aggregation_option == 'uniform':\n",
    "            return fmodule._model_average(models)\n",
    "        elif self.aggregation_option == 'weighted_com':\n",
    "            p = [1.0 * local_data_vols[cid] / total_data_vol for cid in self.received_clients]\n",
    "            w = fmodule._model_sum([model_k * pk for model_k, pk in zip(models, p)])\n",
    "            return (1.0-sum(p))*self.model + w\n",
    "        else:\n",
    "            p = [1.0 * local_data_vols[cid] / total_data_vol for cid in self.received_clients]\n",
    "            sump = sum(p)\n",
    "            p = [pk/sump for pk in p]\n",
    "            return fmodule._model_sum([model_k * pk for model_k, pk in zip(models, p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f34e60",
   "metadata": {},
   "source": [
    "We will show how to modify each steps to realize different algorithms by the following sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
